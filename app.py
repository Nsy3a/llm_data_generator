import streamlit as st
import os
import json
import pandas as pd
import time
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
import google.generativeai as genai

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================== åç«¯é€»è¾‘ï¼šå¤šæ¨¡å‹é€‚é…å™¨ ==================

class LLMClient:
    def __init__(self, provider, api_key, base_url=None, model_name=None):
        self.provider = provider
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name

    def generate(self, system_prompt, user_prompt):
        """ç»Ÿä¸€çš„ç”Ÿæˆæ¥å£ï¼Œå±è”½ä¸åŒå‚å•† SDK çš„å·®å¼‚"""
        try:
            if self.provider == "OpenAI" or self.provider == "Custom":
                client = OpenAI(api_key=self.api_key, base_url=self.base_url)
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    response_format={"type": "json_object"} # å°è¯•å¼ºåˆ¶JSONæ¨¡å¼
                )
                return response.choices[0].message.content

            elif self.provider == "Anthropic":
                client = anthropic.Anthropic(api_key=self.api_key)
                message = client.messages.create(
                    model=self.model_name,
                    max_tokens=4096,
                    temperature=0.7,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                return message.content[0].text

            elif self.provider == "Google":
                genai.configure(api_key=self.api_key)
                model = genai.GenerativeModel(
                    self.model_name,
                    generation_config={"response_mime_type": "application/json"}
                )
                # Gemini system prompt éœ€è¦åœ¨å®ä¾‹åŒ–æ—¶é…ç½®æˆ–æ‹¼æ¥åˆ° user promptï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                full_prompt = f"System Instruction:\n{system_prompt}\n\nUser Task:\n{user_prompt}"
                response = model.generate_content(full_prompt)
                return response.text

        except Exception as e:
            return f"Error: {str(e)}"

def clean_json_text(text):
    """æ¸…æ´— JSON å­—ç¬¦ä¸²ï¼Œç§»é™¤ Markdown æ ‡è®°"""
    import re
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
    return text.strip()

# ================== å‰ç«¯ç•Œé¢ (Streamlit) ==================

st.set_page_config(page_title="AI æ•°æ®é›†è’¸é¦å·¥å‚", layout="wide", page_icon="ğŸ­")

st.title("ğŸ­ é«˜è´¨é‡æ•°æ®é›†è’¸é¦å·¥å‚")
st.markdown("åˆ©ç”¨å¼ºå¤§çš„å¤§æ¨¡å‹ï¼ˆTeacher Modelï¼‰ç”Ÿæˆç”¨äºå¾®è°ƒï¼ˆSFTï¼‰çš„é«˜è´¨é‡æŒ‡ä»¤æ•°æ®é›†ã€‚")

# --- ä¾§è¾¹æ ï¼šæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ æ¨¡å‹è®¾ç½®")
    
    provider = st.selectbox(
        "é€‰æ‹©æ¨¡å‹æœåŠ¡å•†",
        ["OpenAI", "Anthropic", "Google", "Custom"]
    )

    api_key = ""
    base_url = None
    model_name = ""

    # åŠ¨æ€æ˜¾ç¤ºé…ç½®é¡¹ï¼Œä¼˜å…ˆè¯»å– .env
    if provider == "OpenAI":
        api_key = st.text_input("API Key", value=os.getenv("OPENAI_API_KEY", ""), type="password")
        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"])
    
    elif provider == "Anthropic":
        api_key = st.text_input("API Key", value=os.getenv("ANTHROPIC_API_KEY", ""), type="password")
        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["claude-3-5-sonnet-20240620", "claude-3-opus-20240229"])
        
    elif provider == "Google":
        api_key = st.text_input("API Key", value=os.getenv("GOOGLE_API_KEY", ""), type="password")
        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gemini-1.5-pro", "gemini-1.5-flash"])
        
    elif provider == "Custom":
        st.info("é€‚ç”¨äº DeepSeek, Groq, Moonshot æˆ– æœ¬åœ° vLLM/Ollama")
        base_url = st.text_input("Base URL", value=os.getenv("CUSTOM_BASE_URL", "https://api.openai.com/v1"))
        api_key = st.text_input("API Key", value=os.getenv("CUSTOM_API_KEY", "sk-xxxx"), type="password")
        model_name = st.text_input("Model Name", value="llama3-70b")

    if not api_key:
        st.warning("âš ï¸ è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®å¯†é’¥æˆ–åœ¨ä¸Šæ–¹è¾“å…¥")

# --- ä¸»ç•Œé¢é€»è¾‘ ---

# åˆå§‹åŒ– Session State (ç”¨äºä¿å­˜ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ•°æ®)
if "topics" not in st.session_state:
    st.session_state.topics = []
if "generated_data" not in st.session_state:
    st.session_state.generated_data = []

# åŒºåŸŸ 1: é¢†åŸŸå®šä¹‰
st.subheader("1. å®šä¹‰ç›®æ ‡é¢†åŸŸä¸ä»»åŠ¡")
col1, col2 = st.columns([3, 1])
with col1:
    target_domain = st.text_input("è¯·è¾“å…¥ä½ æƒ³å¤åˆ»çš„é¢†åŸŸèƒ½åŠ›", placeholder="ä¾‹å¦‚ï¼šPythonå®‰å…¨ä»£ç å®¡è®¡ã€åŒ»ç–—é—®è¯Šå¯¹è¯ã€åˆä¸­æ•°å­¦å‡ ä½•æ¨ç†")
with col2:
    num_topics = st.number_input("ç”Ÿæˆä¸»é¢˜æ•°é‡", min_value=1, max_value=50, value=5)

if st.button("ğŸš€ ç”Ÿæˆä»»åŠ¡åˆ†ç±»æ ‘ (Taxonomy)"):
    if not api_key:
        st.error("è¯·å…ˆé…ç½® API Key")
    else:
        client = LLMClient(provider, api_key, base_url, model_name)
        with st.spinner(f"æ­£åœ¨è®© {model_name} åˆ†æé¢†åŸŸçŸ¥è¯†..."):
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“å®¶çº§æ•°æ®æ¶æ„å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥çš„é¢†åŸŸï¼Œæ‹†è§£å‡ºå…·ä½“çš„ç»†åˆ†ä»»åŠ¡åœºæ™¯ã€‚"
            user_prompt = f"""
            é¢†åŸŸï¼š{target_domain}
            è¯·ç”Ÿæˆ {num_topics} ä¸ªå…·ä½“çš„ã€é«˜éš¾åº¦çš„ç»†åˆ†ä»»åŠ¡ã€‚
            è¦æ±‚ï¼šè¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼ŒåŒ…å« 'topics' åˆ—è¡¨ã€‚
            
            JSON ç¤ºä¾‹ï¼š
            {{ "topics": ["ä»»åŠ¡A", "ä»»åŠ¡B", "ä»»åŠ¡C"] }}
            """
            
            raw_resp = client.generate(system_prompt, user_prompt)
            
            try:
                cleaned_resp = clean_json_text(raw_resp)
                data = json.loads(cleaned_resp)
                st.session_state.topics = data.get("topics", [])
                st.success(f"æˆåŠŸç”Ÿæˆ {len(st.session_state.topics)} ä¸ªä»»åŠ¡ä¸»é¢˜ï¼")
            except Exception as e:
                st.error(f"è§£æå¤±è´¥: {e}")
                st.text(raw_resp)

# æ˜¾ç¤ºå·²ç”Ÿæˆçš„ä¸»é¢˜
if st.session_state.topics:
    st.info(f"å½“å‰å¾…ç”Ÿæˆä¸»é¢˜ï¼š{', '.join(st.session_state.topics)}")

    st.divider()

    # åŒºåŸŸ 2: æ•°æ®ç”Ÿæˆ
    st.subheader("2. æ‰¹é‡ç”Ÿäº§é«˜è´¨é‡æ•°æ®")
    
    samples_per_topic = st.slider("æ¯ä¸ªä¸»é¢˜ç”Ÿæˆçš„æ•°æ®é‡", 1, 20, 3)
    
    if st.button("ğŸ”¥ å¼€å§‹è’¸é¦æ•°æ®"):
        client = LLMClient(provider, api_key, base_url, model_name)
        st.session_state.generated_data = [] # æ¸…ç©ºæ—§æ•°æ®
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_steps = len(st.session_state.topics)
        
        for i, topic in enumerate(st.session_state.topics):
            status_text.text(f"æ­£åœ¨ç”Ÿæˆä¸»é¢˜ ({i+1}/{total_steps}): {topic} ...")
            
            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºé«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„AIã€‚
            ä¸¥æ ¼è¦æ±‚ï¼š
            1. Outputå¿…é¡»åŒ…å« "Thought" (æ€ç»´é“¾) å’Œ "Answer"ã€‚
            2. æ ¼å¼å¿…é¡»æ˜¯åˆæ³•çš„ JSON åˆ—è¡¨ã€‚
            """
            
            user_prompt = f"""
            ä¸»é¢˜ï¼š{topic}
            è¯·ç”Ÿæˆ {samples_per_topic} æ¡å¤æ‚çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚
            
            è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
            {{
                "samples": [
                    {{
                        "instruction": "ç”¨æˆ·æŒ‡ä»¤...",
                        "input": "ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰...",
                        "output": "Thought: ... Answer: ..."
                    }}
                ]
            }}
            """
            
            raw_resp = client.generate(system_prompt, user_prompt)
            
            try:
                cleaned_resp = clean_json_text(raw_resp)
                batch_data = json.loads(cleaned_resp).get("samples", [])
                
                for item in batch_data:
                    item['category'] = topic # æ·»åŠ å…ƒæ•°æ®
                    st.session_state.generated_data.append(item)
                    
            except Exception as e:
                st.warning(f"ä¸»é¢˜ {topic} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            
            # æ›´æ–°è¿›åº¦
            progress_bar.progress((i + 1) / total_steps)
            time.sleep(0.5) # é¿å…é€Ÿç‡é™åˆ¶

        status_text.text("âœ… æ‰€æœ‰æ•°æ®ç”Ÿæˆå®Œæ¯•ï¼")
        
    # åŒºåŸŸ 3: ç»“æœå±•ç¤ºä¸ä¸‹è½½
    if st.session_state.generated_data:
        st.subheader("3. æ•°æ®é›†é¢„è§ˆä¸å¯¼å‡º")
        
        df = pd.DataFrame(st.session_state.generated_data)
        st.dataframe(df, use_container_width=True)
        
        # è½¬æ¢ä¸º JSONL æ ¼å¼ä¾›ä¸‹è½½
        jsonl_data = df.to_json(orient="records", lines=True, force_ascii=False)
        
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½ JSONL æ ¼å¼æ•°æ®é›† (å¯ç›´æ¥ç”¨äºè®­ç»ƒ)",
            data=jsonl_data,
            file_name=f"dataset_{target_domain}.jsonl",
            mime="application/json"
        )
        
        # CSV ä¸‹è½½é€‰é¡¹
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½ CSV æ ¼å¼ (ExcelæŸ¥çœ‹)",
            data=csv_data,
            file_name=f"dataset_{target_domain}.csv",
            mime="text/csv"
        )
