import streamlit as st
import os
import json
import pandas as pd
import time
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
import google.generativeai as genai

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================== åç«¯é€»è¾‘ï¼šå¤šæ¨¡å‹é€‚é…å™¨ ==================

class LLMClient:
    def __init__(self, provider, api_key, base_url=None, model_name=None):
        self.provider = provider
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name

    def generate(self, system_prompt, user_prompt):
        """ç»Ÿä¸€çš„ç”Ÿæˆæ¥å£ï¼Œå±è”½ä¸åŒå‚å•† SDK çš„å·®å¼‚"""
        try:
            if self.provider == "OpenAI" or self.provider == "Custom (OpenAI-Compatible)":
                client = OpenAI(api_key=self.api_key, base_url=self.base_url)
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    response_format={"type": "json_object"} # å°è¯•å¼ºåˆ¶JSONæ¨¡å¼
                )
                return response.choices[0].message.content

            elif self.provider == "Anthropic (Claude)":
                client = anthropic.Anthropic(api_key=self.api_key)
                message = client.messages.create(
                    model=self.model_name,
                    max_tokens=4096,
                    temperature=0.7,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                return message.content[0].text

            elif self.provider == "Google (Gemini)":
                genai.configure(api_key=self.api_key)
                model = genai.GenerativeModel(
                    self.model_name,
                    generation_config={"response_mime_type": "application/json"}
                )
                # Gemini system prompt éœ€è¦åœ¨å®ä¾‹åŒ–æ—¶é…ç½®æˆ–æ‹¼æ¥åˆ° user promptï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                full_prompt = f"System Instruction:\n{system_prompt}\n\nUser Task:\n{user_prompt}"
                response = model.generate_content(full_prompt)
                return response.text

        except Exception as e:
            return f"Error: {str(e)}"

def clean_json_text(text):
    """æ¸…æ´— JSON å­—ç¬¦ä¸²ï¼Œç§»é™¤ Markdown æ ‡è®°"""
    import re
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
    return text.strip()

# ================== å‰ç«¯ç•Œé¢ (Streamlit) ==================

st.set_page_config(page_title="AI æ•°æ®é›†è’¸é¦å·¥å‚", layout="wide", page_icon="ğŸ­")

# æ·»åŠ è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
/* ä¼˜åŒ–ä¾§è¾¹æ æ ·å¼ */
.css-1d391kg {
    padding: 1rem 1.5rem;
}

/* ä¼˜åŒ–æ ‡é¢˜æ ·å¼ */
.stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {
    color: #1f4788;
    font-weight: 600;
}

/* ä¼˜åŒ–å®¹å™¨æ ·å¼ */
.stContainer {
    border: 1px solid #e0e0e0;
    border-radius: 8px;
    padding: 1rem;
    margin: 0.5rem 0;
    background-color: #f8f9fa;
}

/* ä¼˜åŒ–æŒ‰é’®æ ·å¼ */
.stButton > button {
    border-radius: 8px;
    font-weight: 500;
    transition: all 0.3s ease;
}

.stButton > button:hover {
    transform: translateY(-1px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}

/* ä¼˜åŒ–é€‰æ‹©æ¡†æ ·å¼ */
.stSelectbox > div > div {
    border-radius: 8px;
}

/* ä¼˜åŒ–è¾“å…¥æ¡†æ ·å¼ */
.stTextInput > div > div > input {
    border-radius: 8px;
    border: 1px solid #ccc;
}

/* ä¼˜åŒ–è­¦å‘Šå’ŒæˆåŠŸæ¶ˆæ¯ */
.stAlert {
    border-radius: 8px;
    padding: 1rem;
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ­ é«˜è´¨é‡æ•°æ®é›†è’¸é¦å·¥å‚")
st.markdown("åˆ©ç”¨å¼ºå¤§çš„å¤§æ¨¡å‹ï¼ˆTeacher Modelï¼‰ç”Ÿæˆç”¨äºå¾®è°ƒï¼ˆSFTï¼‰çš„é«˜è´¨é‡æŒ‡ä»¤æ•°æ®é›†ã€‚")

# --- ä¾§è¾¹æ ï¼šæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ æ¨¡å‹è®¾ç½®")
    
    # æ·»åŠ åˆ†éš”çº¿å’Œè¯´æ˜
    st.markdown("---")
    st.markdown("ğŸ¯ **é€‰æ‹©AIæ¨¡å‹æœåŠ¡å•†**")
    
    provider = st.selectbox(
        "æœåŠ¡å•†",
        ["OpenAI", "Anthropic (Claude)", "Google (Gemini)", "Custom (OpenAI-Compatible)"],
        help="é€‰æ‹©æ‚¨è¦ä½¿ç”¨çš„AIæ¨¡å‹æœåŠ¡å•†"
    )

    api_key = ""
    base_url = None
    model_name = ""

    # åŠ¨æ€æ˜¾ç¤ºé…ç½®é¡¹ï¼Œä¼˜å…ˆè¯»å– .env
    if provider == "OpenAI":
        with st.container():
            st.markdown("**ğŸ”‘ è®¤è¯é…ç½®**")
            api_key = st.text_input("API Key", value=os.getenv("OPENAI_API_KEY", ""), type="password", help="æ‚¨çš„OpenAI APIå¯†é’¥")
            
            st.markdown("**ğŸ¤– æ¨¡å‹é…ç½®**")
            model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"], help="é€‰æ‹©OpenAIæ¨¡å‹")
    
    elif provider == "Anthropic (Claude)":
        with st.container():
            st.markdown("**ğŸ”‘ è®¤è¯é…ç½®**")
            api_key = st.text_input("API Key", value=os.getenv("ANTHROPIC_API_KEY", ""), type="password", help="æ‚¨çš„Anthropic APIå¯†é’¥")
            
            st.markdown("**ğŸ¤– æ¨¡å‹é…ç½®**")
            model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["claude-3-5-sonnet-20240620", "claude-3-opus-20240229"], help="é€‰æ‹©Claudeæ¨¡å‹")
        
    elif provider == "Google (Gemini)":
        with st.container():
            st.markdown("**ğŸ”‘ è®¤è¯é…ç½®**")
            api_key = st.text_input("API Key", value=os.getenv("GOOGLE_API_KEY", ""), type="password", help="æ‚¨çš„Google APIå¯†é’¥")
            
            st.markdown("**ğŸ¤– æ¨¡å‹é…ç½®**")
            model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gemini-1.5-pro", "gemini-1.5-flash"], help="é€‰æ‹©Geminiæ¨¡å‹")
        
    elif provider == "Custom (OpenAI-Compatible)":
        st.info("ğŸ’¡ é€‚ç”¨äº DeepSeek, Groq, Moonshot æˆ– æœ¬åœ° vLLM/Ollama")
        
        # ç§»é™¤å®¹å™¨åŒ…è£…ï¼Œé¿å…æ‚¬åœæ—¶å‡ºç°å¤šä½™å®¹å™¨
        # ä½¿ç”¨å®¹å™¨æ¥ç»„ç»‡è‡ªå®šä¹‰é…ç½®
        st.markdown("**ğŸ”§ æœåŠ¡å™¨é…ç½®**")
        
        # ç»Ÿä¸€çš„URLè¾“å…¥æ¡†ï¼ˆè‡ªåŠ¨è§£æç«¯å£ï¼‰
        base_url = st.text_input(
            "Base URL", 
            value=os.getenv("CUSTOM_BASE_URL", "https://api.openai.com/v1"), 
            help="å®Œæ•´çš„APIæœåŠ¡å™¨åœ°å€ï¼Œå¦‚ï¼šhttps://api.deepseek.com/v1 æˆ– http://localhost:8000/v1"
        )
        
        # URLè§£æå’ŒéªŒè¯æç¤º
        if base_url:
            import re
            url_pattern = r'^(https?://)([^:/]+)(:(\d+))?(/.*)?$'
            match = re.match(url_pattern, base_url)
            
            if match:
                protocol, domain, _, port, path = match.groups()
                port_display = f":{port}" if port else ""
                st.info(f"ğŸ“ è§£æç»“æœ: {protocol}{domain}{port_display}{path or ''}")
            else:
                st.warning("âš ï¸ URLæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥è¾“å…¥")
        
        st.markdown("**ğŸ”‘ è®¤è¯é…ç½®**")
        
        # ç¬¬äºŒè¡Œï¼šAPIå¯†é’¥ï¼ˆå¸¦æ˜¾éšåˆ‡æ¢ï¼‰
        col_key, col_toggle = st.columns([4, 1])
        with col_key:
            if 'show_custom_key' not in st.session_state:
                st.session_state.show_custom_key = False
            
            key_type = "text" if st.session_state.show_custom_key else "password"
            api_key = st.text_input(
                "API Key", 
                value=os.getenv("CUSTOM_API_KEY", "sk-xxxx"), 
                type=key_type,
                help="æ‚¨çš„APIè®¿é—®å¯†é’¥"
            )
        
        with col_toggle:
            st.write("")  # æ·»åŠ ç©ºè¡Œå¯¹é½
            if st.button("ğŸ‘ï¸" if not st.session_state.show_custom_key else "ğŸ™ˆ", 
                       help="æ˜¾ç¤º/éšè—å¯†é’¥",
                       key="toggle_custom_key"):
                st.session_state.show_custom_key = not st.session_state.show_custom_key
                st.rerun()
        
        st.markdown("**ğŸ¤– æ¨¡å‹é…ç½®**")
        
        # æ¨¡å‹é€‰æ‹© - åˆ†ç¦»å¸ƒå±€
        # ç¬¬ä¸€è¡Œï¼šæ¨¡å‹é€‰æ‹©ä¸‹æ‹‰èœå•
        preset_models = [
            "llama3-70b", "llama3-8b", "deepseek-chat", "deepseek-coder", 
            "mixtral-8x7b", "qwen-14b", "gpt-3.5-turbo", "gpt-4"
        ]
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹", 
            preset_models + ["è‡ªå®šä¹‰"],
            help="é€‰æ‹©é¢„è®¾æ¨¡å‹æˆ–è‡ªå®šä¹‰è¾“å…¥"
        )
        
        # ç¬¬äºŒè¡Œï¼šæ¨¡å‹æ˜¾ç¤ºæˆ–è‡ªå®šä¹‰è¾“å…¥
        if selected_model == "è‡ªå®šä¹‰":
            model_name = st.text_input(
                "è‡ªå®šä¹‰æ¨¡å‹åç§°", 
                value="custom-model",
                help="è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰æ¨¡å‹åç§°"
            )
        else:
            # æ˜¾ç¤ºå½“å‰é€‰ä¸­çš„æ¨¡å‹ï¼ˆåªè¯»ï¼‰
            st.text_input(
                "å½“å‰æ¨¡å‹", 
                value=selected_model, 
                disabled=True,
                help="å½“å‰é€‰ä¸­çš„æ¨¡å‹"
            )
            model_name = selected_model
    
    # å¿«é€Ÿé…ç½®æç¤º
    with st.expander("ğŸ“š å¿«é€Ÿé…ç½®æŒ‡å—"):
            st.markdown("""
            **å¸¸ç”¨é…ç½®ç¤ºä¾‹ï¼š**
            - **DeepSeek**: `https://api.deepseek.com/v1` + `deepseek-chat`
            - **Groq**: `https://api.groq.com/openai/v1` + `mixtral-8x7b`
            - **Moonshot**: `https://api.moonshot.cn/v1` + `moonshot-v1-8k`
            - **æœ¬åœ°vLLM**: `http://localhost:8000/v1` + è‡ªå®šä¹‰æ¨¡å‹å
            - **æœ¬åœ°Ollama**: `http://localhost:11434/v1` + è‡ªå®šä¹‰æ¨¡å‹å
            """)

    # æ·»åŠ åˆ†éš”çº¿å’ŒçŠ¶æ€æç¤º
    st.markdown("---")
    
    if not api_key:
        st.warning("âš ï¸ è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®å¯†é’¥æˆ–åœ¨ä¸Šæ–¹è¾“å…¥", icon="ğŸ”‘")
    else:
        st.success(f"âœ… {provider} é…ç½®å°±ç»ª", icon="âœ¨")

# --- ä¸»ç•Œé¢é€»è¾‘ ---

# åˆå§‹åŒ– Session State (ç”¨äºä¿å­˜ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ•°æ®)
if "topics" not in st.session_state:
    st.session_state.topics = []
if "generated_data" not in st.session_state:
    st.session_state.generated_data = []

# åŒºåŸŸ 1: é¢†åŸŸå®šä¹‰
st.subheader("1. å®šä¹‰ç›®æ ‡é¢†åŸŸä¸ä»»åŠ¡")
col1, col2 = st.columns([3, 1])
with col1:
    target_domain = st.text_input("è¯·è¾“å…¥ä½ æƒ³å¤åˆ»çš„é¢†åŸŸèƒ½åŠ›", placeholder="ä¾‹å¦‚ï¼šPythonå®‰å…¨ä»£ç å®¡è®¡ã€åŒ»ç–—é—®è¯Šå¯¹è¯ã€åˆä¸­æ•°å­¦å‡ ä½•æ¨ç†")
with col2:
    num_topics = st.number_input("ç”Ÿæˆä¸»é¢˜æ•°é‡", min_value=1, max_value=50, value=5)

if st.button("ğŸš€ ç”Ÿæˆä»»åŠ¡åˆ†ç±»æ ‘ (Taxonomy)"):
    if not api_key:
        st.error("è¯·å…ˆé…ç½® API Key")
    else:
        client = LLMClient(provider, api_key, base_url, model_name)
        with st.spinner(f"æ­£åœ¨è®© {model_name} åˆ†æé¢†åŸŸçŸ¥è¯†..."):
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“å®¶çº§æ•°æ®æ¶æ„å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥çš„é¢†åŸŸï¼Œæ‹†è§£å‡ºå…·ä½“çš„ç»†åˆ†ä»»åŠ¡åœºæ™¯ã€‚"
            user_prompt = f"""
            é¢†åŸŸï¼š{target_domain}
            è¯·ç”Ÿæˆ {num_topics} ä¸ªå…·ä½“çš„ã€é«˜éš¾åº¦çš„ç»†åˆ†ä»»åŠ¡ã€‚
            è¦æ±‚ï¼šè¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼ŒåŒ…å« 'topics' åˆ—è¡¨ã€‚
            
            JSON ç¤ºä¾‹ï¼š
            {{ "topics": ["ä»»åŠ¡A", "ä»»åŠ¡B", "ä»»åŠ¡C"] }}
            """
            
            raw_resp = client.generate(system_prompt, user_prompt)
            
            try:
                cleaned_resp = clean_json_text(raw_resp)
                data = json.loads(cleaned_resp)
                st.session_state.topics = data.get("topics", [])
                st.success(f"æˆåŠŸç”Ÿæˆ {len(st.session_state.topics)} ä¸ªä»»åŠ¡ä¸»é¢˜ï¼")
            except Exception as e:
                st.error(f"è§£æå¤±è´¥: {e}")
                st.text(raw_resp)

# æ˜¾ç¤ºå·²ç”Ÿæˆçš„ä¸»é¢˜
if st.session_state.topics:
    st.info(f"å½“å‰å¾…ç”Ÿæˆä¸»é¢˜ï¼š{', '.join(st.session_state.topics)}")

    st.divider()

    # åŒºåŸŸ 2: æ•°æ®ç”Ÿæˆ
    st.subheader("2. æ‰¹é‡ç”Ÿäº§é«˜è´¨é‡æ•°æ®")
    
    samples_per_topic = st.slider("æ¯ä¸ªä¸»é¢˜ç”Ÿæˆçš„æ•°æ®é‡", 1, 20, 3)
    
    if st.button("ğŸ”¥ å¼€å§‹è’¸é¦æ•°æ®"):
        client = LLMClient(provider, api_key, base_url, model_name)
        st.session_state.generated_data = [] # æ¸…ç©ºæ—§æ•°æ®
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_steps = len(st.session_state.topics)
        
        for i, topic in enumerate(st.session_state.topics):
            status_text.text(f"æ­£åœ¨ç”Ÿæˆä¸»é¢˜ ({i+1}/{total_steps}): {topic} ...")
            
            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºé«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„AIã€‚
            ä¸¥æ ¼è¦æ±‚ï¼š
            1. Outputå¿…é¡»åŒ…å« "Thought" (æ€ç»´é“¾) å’Œ "Answer"ã€‚
            2. æ ¼å¼å¿…é¡»æ˜¯åˆæ³•çš„ JSON åˆ—è¡¨ã€‚
            """
            
            user_prompt = f"""
            ä¸»é¢˜ï¼š{topic}
            è¯·ç”Ÿæˆ {samples_per_topic} æ¡å¤æ‚çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚
            
            è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
            {{
                "samples": [
                    {{
                        "instruction": "ç”¨æˆ·æŒ‡ä»¤...",
                        "input": "ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰...",
                        "output": "Thought: ... Answer: ..."
                    }}
                ]
            }}
            """
            
            raw_resp = client.generate(system_prompt, user_prompt)
            
            try:
                cleaned_resp = clean_json_text(raw_resp)
                batch_data = json.loads(cleaned_resp).get("samples", [])
                
                for item in batch_data:
                    item['category'] = topic # æ·»åŠ å…ƒæ•°æ®
                    st.session_state.generated_data.append(item)
                    
            except Exception as e:
                st.warning(f"ä¸»é¢˜ {topic} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            
            # æ›´æ–°è¿›åº¦
            progress_bar.progress((i + 1) / total_steps)
            time.sleep(0.5) # é¿å…é€Ÿç‡é™åˆ¶

        status_text.text("âœ… æ‰€æœ‰æ•°æ®ç”Ÿæˆå®Œæ¯•ï¼")
        
    # åŒºåŸŸ 3: ç»“æœå±•ç¤ºä¸ä¸‹è½½
    if st.session_state.generated_data:
        st.subheader("3. æ•°æ®é›†é¢„è§ˆä¸å¯¼å‡º")
        
        df = pd.DataFrame(st.session_state.generated_data)
        st.dataframe(df, use_container_width=True)
        
        # è½¬æ¢ä¸º JSONL æ ¼å¼ä¾›ä¸‹è½½
        jsonl_data = df.to_json(orient="records", lines=True, force_ascii=False)
        
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½ JSONL æ ¼å¼æ•°æ®é›† (å¯ç›´æ¥ç”¨äºè®­ç»ƒ)",
            data=jsonl_data,
            file_name=f"dataset_{target_domain}.jsonl",
            mime="application/json"
        )
        
        # CSV ä¸‹è½½é€‰é¡¹
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½ CSV æ ¼å¼ (ExcelæŸ¥çœ‹)",
            data=csv_data,
            file_name=f"dataset_{target_domain}.csv",
            mime="text/csv"
        )
