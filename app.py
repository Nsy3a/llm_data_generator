import streamlit as st
import os
import json
import pandas as pd
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
import google.generativeai as genai

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================== åç«¯é€»è¾‘ï¼šå¤šæ¨¡å‹é€‚é…å™¨ ==================

class PollinationsAIClient:
    """Pollinations AI å®¢æˆ·ç«¯ - æ”¯æŒæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆ"""
    def __init__(self, model_name="openai", model_type="text"):
        self.base_url = "https://text.pollinations.ai" if model_type == "text" else "https://image.pollinations.ai"
        self.model_name = model_name
        self.model_type = model_type
    
    def generate_text(self, prompt, system_prompt=None, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬ - ä½¿ç”¨Pollinations AIæ–‡æœ¬API"""
        import requests
        import urllib.parse
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\nç”¨æˆ·ä»»åŠ¡ï¼š{prompt}"
        
        # URLç¼–ç æç¤ºè¯
        encoded_prompt = urllib.parse.quote(full_prompt)
        
        # æ„å»ºè¯·æ±‚URL
        url = f"{self.base_url}/prompt/{encoded_prompt}"
        
        # æ·»åŠ å‚æ•°
        params = {"model": self.model_name}
        if kwargs.get("seed"):
            params["seed"] = kwargs["seed"]
        if kwargs.get("private"):
            params["private"] = "true"
        
        try:
            response = requests.get(url, params=params, timeout=60)
            response.raise_for_status()
            return response.text
        except Exception as e:
            return f"Error: {str(e)}"
    
    def generate_image(self, prompt, **kwargs):
        """ç”Ÿæˆå›¾åƒ - ä½¿ç”¨Pollinations AIå›¾åƒAPI"""
        import requests
        import urllib.parse
        
        # URLç¼–ç æç¤ºè¯
        encoded_prompt = urllib.parse.quote(prompt)
        
        # æ„å»ºè¯·æ±‚URL
        url = f"{self.base_url}/prompt/{encoded_prompt}"
        
        # æ·»åŠ å‚æ•°
        params = {
            "model": kwargs.get("model", "flux"),
            "width": kwargs.get("width", 1024),
            "height": kwargs.get("height", 1024),
            "seed": kwargs.get("seed"),
            "nologo": "true" if kwargs.get("nologo", False) else "false",
            "private": "true" if kwargs.get("private", True) else "false",
            "enhance": "true" if kwargs.get("enhance", False) else "false"
        }
        
        # ç§»é™¤Noneå€¼
        params = {k: v for k, v in params.items() if v is not None}
        
        try:
            response = requests.get(url, params=params, timeout=300)
            response.raise_for_status()
            return response.content  # è¿”å›å›¾åƒäºŒè¿›åˆ¶æ•°æ®
        except Exception as e:
            return f"Error: {str(e)}"

class LLMClient:
    def __init__(self, provider, api_key, base_url=None, model_name=None):
        self.provider = provider
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name

    def generate(self, system_prompt, user_prompt):
        """ç»Ÿä¸€çš„ç”Ÿæˆæ¥å£ï¼Œå±è”½ä¸åŒå‚å•† SDK çš„å·®å¼‚"""
        try:
            if self.provider == "OpenAI":
                client = OpenAI(api_key=self.api_key, base_url=self.base_url)
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    response_format={"type": "json_object"} # å°è¯•å¼ºåˆ¶JSONæ¨¡å¼
                )
                return response.choices[0].message.content

            elif self.provider == "Custom":
                if not self.base_url or not self.model_name:
                    return f"Error: Custom provider requires both base_url and model_name to be configured"
                client = OpenAI(api_key=self.api_key, base_url=self.base_url)
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    response_format={"type": "json_object"} # å°è¯•å¼ºåˆ¶JSONæ¨¡å¼
                )
                return response.choices[0].message.content

            elif self.provider == "Anthropic":
                client = anthropic.Anthropic(api_key=self.api_key)
                message = client.messages.create(
                    model=self.model_name,
                    max_tokens=4096,
                    temperature=0.7,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                return message.content[0].text

            elif self.provider == "Google":
                genai.configure(api_key=self.api_key)
                model = genai.GenerativeModel(
                    self.model_name,
                    generation_config={"response_mime_type": "application/json"}
                )
                # Gemini system prompt éœ€è¦åœ¨å®ä¾‹åŒ–æ—¶é…ç½®æˆ–æ‹¼æ¥åˆ° user promptï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
                full_prompt = f"System Instruction:\n{system_prompt}\n\nUser Task:\n{user_prompt}"
                response = model.generate_content(full_prompt)
                return response.text

            elif self.provider == "Pollinations":
                # ä½¿ç”¨Pollinations AIç”Ÿæˆæ–‡æœ¬
                client = PollinationsAIClient(model_name=self.model_name, model_type="text")
                # æ„å»ºé¢å¤–å‚æ•°
                kwargs = {}
                if hasattr(self, 'seed') and self.seed:
                    kwargs['seed'] = self.seed
                if hasattr(self, 'private') and self.private:
                    kwargs['private'] = self.private
                return client.generate_text(user_prompt, system_prompt, **kwargs)

        except Exception as e:
            return f"Error: {str(e)}"

def clean_json_text(text):
    """æ¸…æ´— JSON å­—ç¬¦ä¸²ï¼Œç§»é™¤ Markdown æ ‡è®°"""
    import re
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
    return text.strip()

# ================== å‰ç«¯ç•Œé¢ (Streamlit) ==================

st.set_page_config(page_title="AI æ•°æ®é›†è’¸é¦å·¥å‚", layout="wide", page_icon="ğŸ­")

# è‡ªå®šä¹‰CSSæ¥è°ƒæ•´å¯†ç è¾“å…¥æ¡†æ ·å¼
st.markdown("""
<style>
/* å¯†ç è¾“å…¥æ¡†æ ·å¼è°ƒæ•´ */
.stTextInput > div {
    position: relative !important;
}

.stTextInput > div > div {
    position: relative !important;
}

.stTextInput input[type="password"] {
    right: 0px !important;
    position: relative !important;
}

/* è°ƒæ•´å°çœ¼ç›æŒ‰é’®ä½ç½®ï¼Œç»§ç»­å¾€å³ç§»åŠ¨ */
.stTextInput > div > div > button[title*="password"] {
    right: -12px !important;
    position: relative !important;
}

/* ç¡®ä¿ä¸ä¸‹æ‹‰é€‰æ‹©æ¡†çš„ç®­å¤´å‚ç›´å¯¹é½ */
.stSelectbox > div > div {
    position: relative;
}

/* è°ƒæ•´ä¸‹æ‹‰ç®­å¤´ä½ç½®ï¼Œä¸å¯†ç æ¡†å°çœ¼ç›å›¾æ ‡å‚ç›´å¯¹é½ */
.stSelectbox > div > div > div:last-child {
    right: 0px;
}
</style>
""", unsafe_allow_html=True)

# è·å–OpenAIæ¨¡å‹åˆ—è¡¨çš„å‡½æ•°
def get_openai_models():
    """ä»OpenAI APIè·å–æœ€æ–°çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è·å–æ¨¡å‹åˆ—è¡¨
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))
        models_response = client.models.list()
        
        # ç­›é€‰å‡ºèŠå¤©æ¨¡å‹
        chat_models = []
        for model in models_response.data:
            model_id = model.id
            # ç­›é€‰å‡ºé€‚åˆèŠå¤©çš„æ¨¡å‹
            if any(keyword in model_id for keyword in ["gpt", "chat"]):
                chat_models.append({
                    'display': model_id,
                    'value': model_id,
                    'description': f"OpenAI {model_id}"
                })
        
        # æŒ‰åç§°æ’åº
        chat_models.sort(key=lambda x: x['display'])
        return chat_models
        
    except Exception as e:
        st.warning(f"è·å–OpenAIåœ¨çº¿æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æœ¬åœ°å¤‡ä»½æ•°æ®")
        return get_local_openai_models()

# æœ¬åœ°å¤‡ä»½çš„OpenAIæ¨¡å‹æ•°æ®
def get_local_openai_models():
    """æœ¬åœ°å¤‡ä»½çš„OpenAIæ¨¡å‹æ•°æ®"""
    return [
        {'display': 'gpt-4o', 'value': 'gpt-4o', 'description': 'OpenAI GPT-4o'},
        {'display': 'gpt-4o-mini', 'value': 'gpt-4o-mini', 'description': 'OpenAI GPT-4o Mini'},
        {'display': 'gpt-4-turbo', 'value': 'gpt-4-turbo', 'description': 'OpenAI GPT-4 Turbo'},
        {'display': 'gpt-3.5-turbo', 'value': 'gpt-3.5-turbo', 'description': 'OpenAI GPT-3.5 Turbo'}
    ]

# è·å–Anthropicæ¨¡å‹åˆ—è¡¨çš„å‡½æ•°
def get_anthropic_models():
    """ä»Anthropic APIè·å–æœ€æ–°çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        # ä½¿ç”¨Anthropicå®¢æˆ·ç«¯è·å–æ¨¡å‹åˆ—è¡¨
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY", ""))
        models_response = client.models.list()
        
        # ç­›é€‰å‡ºé€‚åˆå¯¹è¯çš„æ¨¡å‹
        chat_models = []
        for model in models_response.data:
            model_id = model.id
            # ç­›é€‰å‡ºClaudeç³»åˆ—æ¨¡å‹
            if "claude" in model_id:
                chat_models.append({
                    'display': model_id,
                    'value': model_id,
                    'description': f"Anthropic {model_id}"
                })
        
        # æŒ‰åç§°æ’åº
        chat_models.sort(key=lambda x: x['display'])
        return chat_models
        
    except Exception as e:
        st.warning(f"è·å–Anthropicåœ¨çº¿æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æœ¬åœ°å¤‡ä»½æ•°æ®")
        return get_local_anthropic_models()

# æœ¬åœ°å¤‡ä»½çš„Anthropicæ¨¡å‹æ•°æ®
def get_local_anthropic_models():
    """æœ¬åœ°å¤‡ä»½çš„Anthropicæ¨¡å‹æ•°æ®"""
    return [
        {'display': 'claude-3-5-sonnet-20240620', 'value': 'claude-3-5-sonnet-20240620', 'description': 'Anthropic Claude 3.5 Sonnet'},
        {'display': 'claude-3-opus-20240229', 'value': 'claude-3-opus-20240229', 'description': 'Anthropic Claude 3 Opus'},
        {'display': 'claude-3-sonnet-20240229', 'value': 'claude-3-sonnet-20240229', 'description': 'Anthropic Claude 3 Sonnet'},
        {'display': 'claude-3-haiku-20240307', 'value': 'claude-3-haiku-20240307', 'description': 'Anthropic Claude 3 Haiku'}
    ]

# è·å–Google Geminiæ¨¡å‹åˆ—è¡¨çš„å‡½æ•°
def get_google_models():
    """è·å–Google Geminiæ¨¡å‹åˆ—è¡¨ - Google APIä¸æä¾›æ¨¡å‹åˆ—è¡¨æ¥å£ï¼Œä½¿ç”¨æœ¬åœ°é…ç½®"""
    try:
        # Google Generative AIæ²¡æœ‰æä¾›è·å–æ¨¡å‹åˆ—è¡¨çš„API
        # ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨
        return [
            {'display': 'gemini-1.5-pro', 'value': 'gemini-1.5-pro', 'description': 'Google Gemini 1.5 Pro'},
            {'display': 'gemini-1.5-flash', 'value': 'gemini-1.5-flash', 'description': 'Google Gemini 1.5 Flash'},
            {'display': 'gemini-2.0-flash-exp', 'value': 'gemini-2.0-flash-exp', 'description': 'Google Gemini 2.0 Flash Experimental'},
            {'display': 'gemini-2.0-flash-thinking-exp-1219', 'value': 'gemini-2.0-flash-thinking-exp-1219', 'description': 'Google Gemini 2.0 Flash Thinking'}
        ]
    except Exception as e:
        st.warning(f"è·å–Googleæ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æœ¬åœ°å¤‡ä»½æ•°æ®")
        return get_local_google_models()

# æœ¬åœ°å¤‡ä»½çš„Googleæ¨¡å‹æ•°æ®
def get_local_google_models():
    """æœ¬åœ°å¤‡ä»½çš„Googleæ¨¡å‹æ•°æ®"""
    return [
        {'display': 'gemini-1.5-pro', 'value': 'gemini-1.5-pro', 'description': 'Google Gemini 1.5 Pro'},
        {'display': 'gemini-1.5-flash', 'value': 'gemini-1.5-flash', 'description': 'Google Gemini 1.5 Flash'},
        {'display': 'gemini-2.0-flash-exp', 'value': 'gemini-2.0-flash-exp', 'description': 'Google Gemini 2.0 Flash Experimental'},
        {'display': 'gemini-2.0-flash-thinking-exp-1219', 'value': 'gemini-2.0-flash-thinking-exp-1219', 'description': 'Google Gemini 2.0 Flash Thinking'}
    ]

# è·å–Pollinations AIæ¨¡å‹åˆ—è¡¨çš„å‡½æ•°
def get_pollinations_models():
    """ä»Pollinations AI APIè·å–æœ€æ–°çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        response = requests.get("https://text.pollinations.ai/models", timeout=5)
        if response.status_code == 200:
            models_data = response.json()
            # æ„å»ºæ¨¡å‹é€‰æ‹©åˆ—è¡¨ï¼Œæ˜¾ç¤ºå®Œæ•´æè¿°ï¼Œä½¿ç”¨nameä½œä¸ºå®é™…å€¼
            model_options = []
            for model in models_data:
                display_name = model['description']
                model_options.append({
                    'display': display_name,
                    'value': model['name'],
                    'description': model.get('description', model['name'])
                })
            return model_options
        else:
            st.warning("æ— æ³•ä»åœ¨çº¿APIè·å–æ¨¡å‹åˆ—è¡¨ï¼Œä½¿ç”¨æœ¬åœ°å¤‡ä»½æ•°æ®")
            return get_local_pollinations_models()
    except Exception as e:
        st.warning(f"è·å–åœ¨çº¿æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æœ¬åœ°å¤‡ä»½æ•°æ®")
        return get_local_pollinations_models()

# æœ¬åœ°å¤‡ä»½çš„æ¨¡å‹æ•°æ®
def get_local_pollinations_models():
    """æœ¬åœ°å¤‡ä»½çš„æ¨¡å‹æ•°æ®"""
    return [
        {'display': 'DeepSeek V3.1', 'value': 'deepseek', 'description': 'DeepSeek V3.1'},
        {'display': 'Gemini 2.5 Flash Lite', 'value': 'gemini', 'description': 'Gemini 2.5 Flash Lite'},
        {'display': 'Gemini 2.5 Flash Lite with Google Search', 'value': 'gemini-search', 'description': 'Gemini 2.5 Flash Lite with Google Search'},
        {'display': 'Mistral Small 3.2 24B', 'value': 'mistral', 'description': 'Mistral Small 3.2 24B'},
        {'display': 'OpenAI GPT', 'value': 'openai', 'description': 'OpenAI GPT'},
        {'display': 'Llama 3.2 3B', 'value': 'llama', 'description': 'Llama 3.2 3B'},
        {'display': 'LlamaGuard 7B', 'value': 'llamaguard', 'description': 'LlamaGuard 7B'},
        {'display': 'Cohere Command', 'value': 'command', 'description': 'Cohere Command'},
        {'display': 'Unity', 'value': 'unity', 'description': 'Unity'}
    ]

st.title("ğŸ­ é«˜è´¨é‡æ•°æ®é›†è’¸é¦å·¥å‚")
st.markdown("åˆ©ç”¨å¼ºå¤§çš„å¤§æ¨¡å‹ï¼ˆTeacher Modelï¼‰ç”Ÿæˆç”¨äºå¾®è°ƒï¼ˆSFTï¼‰çš„é«˜è´¨é‡æŒ‡ä»¤æ•°æ®é›†ã€‚")

# --- ä¾§è¾¹æ ï¼šæ¨¡å‹é…ç½® ---
with st.sidebar:
    st.header("âš™ï¸ æ¨¡å‹è®¾ç½®")
    
    provider = st.selectbox(
        "é€‰æ‹©æ¨¡å‹æœåŠ¡å•†",
        ["Pollinations", "OpenAI", "Anthropic", "Google", "Custom"],
        index=0
    )

    api_key = ""
    base_url = None
    model_name = ""

    # åŠ¨æ€æ˜¾ç¤ºé…ç½®é¡¹ï¼Œä¼˜å…ˆè¯»å– .env
    if provider == "OpenAI":
        api_key = st.text_input("API Key", value=os.getenv("OPENAI_API_KEY", ""), type="password", placeholder="sk-xxxxxxxxxxxxxxxx...")
        
        # åˆå§‹åŒ–session stateç”¨äºè·Ÿè¸ªæ¨¡å‹åŠ è½½çŠ¶æ€
        if "openai_loading" not in st.session_state:
            st.session_state.openai_loading = False
        if "openai_models" not in st.session_state:
            st.session_state.openai_models = []
        
        # åªæœ‰åœ¨åˆ‡æ¢åˆ°OpenAIä¸”æœ‰APIå¯†é’¥æ—¶æ‰è§¦å‘è‡ªåŠ¨è·å–æ¨¡å‹åˆ—è¡¨
        if api_key and ("last_provider" not in st.session_state or st.session_state.last_provider != "OpenAI"):
            st.session_state.last_provider = "OpenAI"
            st.session_state.openai_loading = True
            
            # ä½¿ç”¨spinneræ˜¾ç¤ºåŠ è½½åŠ¨ç”»
            with st.spinner("ğŸ” æ­£åœ¨è·å–OpenAIæ¨¡å‹åˆ—è¡¨..."):
                try:
                    openai_models = get_openai_models()
                    st.session_state.openai_models = openai_models
                    st.session_state.openai_loading = False
                except Exception as e:
                    st.error(f"è·å–OpenAIæ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                    st.session_state.openai_models = []
                    st.session_state.openai_loading = False
        
        # è·å–OpenAIæ¨¡å‹åˆ—è¡¨
        if api_key:
            try:
                if st.session_state.openai_loading:
                    # å¦‚æœæ­£åœ¨åŠ è½½ï¼Œæ˜¾ç¤ºåŸºç¡€æ¨¡å‹é€‰é¡¹
                    display_names = ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"]
                    selected_display = st.selectbox("é€‰æ‹©æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„OpenAIæ¨¡å‹")
                    model_name = selected_display
                elif st.session_state.openai_models:
                    # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
                    openai_models = st.session_state.openai_models
                    display_names = [model['display'] for model in openai_models]
                    selected_display = st.selectbox("é€‰æ‹©æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„OpenAIæ¨¡å‹")
                    model_name = next(model['value'] for model in openai_models if model['display'] == selected_display)
                else:
                    # å®æ—¶è·å–æ¨¡å‹åˆ—è¡¨
                    openai_models = get_openai_models()
                    display_names = [model['display'] for model in openai_models]
                    selected_display = st.selectbox("é€‰æ‹©æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„OpenAIæ¨¡å‹")
                    model_name = next(model['value'] for model in openai_models if model['display'] == selected_display)
            except Exception as e:
                st.error(f"è·å–OpenAIæ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"])
        else:
            model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"])
    
    elif provider == "Anthropic":
        api_key = st.text_input("API Key", value=os.getenv("ANTHROPIC_API_KEY", ""), type="password", placeholder="sk-ant-xxxxxxxxxxxxx...")
        
        # åˆå§‹åŒ–session stateç”¨äºè·Ÿè¸ªæ¨¡å‹åŠ è½½çŠ¶æ€
        if "anthropic_loading" not in st.session_state:
            st.session_state.anthropic_loading = False
        if "anthropic_models" not in st.session_state:
            st.session_state.anthropic_models = []
        
        # åªæœ‰åœ¨åˆ‡æ¢åˆ°Anthropicä¸”æœ‰APIå¯†é’¥æ—¶æ‰è§¦å‘è‡ªåŠ¨è·å–æ¨¡å‹åˆ—è¡¨
        if api_key and ("last_provider" not in st.session_state or st.session_state.last_provider != "Anthropic"):
            st.session_state.last_provider = "Anthropic"
            st.session_state.anthropic_loading = True
            
            # ä½¿ç”¨spinneræ˜¾ç¤ºåŠ è½½åŠ¨ç”»
            with st.spinner("ğŸ” æ­£åœ¨è·å–Anthropicæ¨¡å‹åˆ—è¡¨..."):
                try:
                    anthropic_models = get_anthropic_models()
                    st.session_state.anthropic_models = anthropic_models
                    st.session_state.anthropic_loading = False
                except Exception as e:
                    st.error(f"è·å–Anthropicæ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                    st.session_state.anthropic_models = []
                    st.session_state.anthropic_loading = False
        
        # è·å–Anthropicæ¨¡å‹åˆ—è¡¨
        if api_key:
            try:
                if st.session_state.anthropic_loading:
                    # å¦‚æœæ­£åœ¨åŠ è½½ï¼Œæ˜¾ç¤ºåŸºç¡€æ¨¡å‹é€‰é¡¹
                    display_names = ["claude-3-5-sonnet-20240620", "claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"]
                    selected_display = st.selectbox("é€‰æ‹©æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„Anthropicæ¨¡å‹")
                    model_name = selected_display
                elif st.session_state.anthropic_models:
                    # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
                    anthropic_models = st.session_state.anthropic_models
                    display_names = [model['display'] for model in anthropic_models]
                    selected_display = st.selectbox("é€‰æ‹©æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„Anthropicæ¨¡å‹")
                    model_name = next(model['value'] for model in anthropic_models if model['display'] == selected_display)
                else:
                    # å®æ—¶è·å–æ¨¡å‹åˆ—è¡¨
                    anthropic_models = get_anthropic_models()
                    display_names = [model['display'] for model in anthropic_models]
                    selected_display = st.selectbox("é€‰æ‹©æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„Anthropicæ¨¡å‹")
                    model_name = next(model['value'] for model in anthropic_models if model['display'] == selected_display)
            except Exception as e:
                st.error(f"è·å–Anthropicæ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["claude-3-5-sonnet-20240620", "claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"])
        else:
            model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["claude-3-5-sonnet-20240620", "claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"])
        
    elif provider == "Google":
        api_key = st.text_input("API Key", value=os.getenv("GOOGLE_API_KEY", ""), type="password", placeholder="AIxxxxxxxxxxxxxxxx...")
        
        # åˆå§‹åŒ–session stateç”¨äºè·Ÿè¸ªæ¨¡å‹åŠ è½½çŠ¶æ€
        if "google_loading" not in st.session_state:
            st.session_state.google_loading = False
        if "google_models" not in st.session_state:
            st.session_state.google_models = []
        
        # åªæœ‰åœ¨åˆ‡æ¢åˆ°Googleæ—¶æ‰è§¦å‘è‡ªåŠ¨è·å–æ¨¡å‹åˆ—è¡¨
        if "last_provider" not in st.session_state or st.session_state.last_provider != "Google":
            st.session_state.last_provider = "Google"
            st.session_state.google_loading = True
            
            # ä½¿ç”¨spinneræ˜¾ç¤ºåŠ è½½åŠ¨ç”»
            with st.spinner("ğŸ” æ­£åœ¨è·å–Googleæ¨¡å‹åˆ—è¡¨..."):
                try:
                    google_models = get_google_models()
                    st.session_state.google_models = google_models
                    st.session_state.google_loading = False
                except Exception as e:
                    st.error(f"è·å–Googleæ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                    st.session_state.google_models = []
                    st.session_state.google_loading = False
        
        # è·å–Googleæ¨¡å‹åˆ—è¡¨
        if st.session_state.google_loading:
            # å¦‚æœæ­£åœ¨åŠ è½½ï¼Œæ˜¾ç¤ºåŸºç¡€æ¨¡å‹é€‰é¡¹
            google_models = [
                {'display': 'Gemini 2.5 Pro', 'value': 'gemini-2.5-pro', 'description': 'Gemini 2.5 Pro'},
                {'display': 'Gemini 2.5 Flash', 'value': 'gemini-2.5-flash', 'description': 'Gemini 2.5 Flash'},
                {'display': 'Gemini 2.0 Flash', 'value': 'gemini-2.0-flash', 'description': 'Gemini 2.0 Flash'},
                {'display': 'Gemini 1.5 Pro', 'value': 'gemini-1.5-pro', 'description': 'Gemini 1.5 Pro'}
            ]
        elif st.session_state.google_models:
            # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
            google_models = st.session_state.google_models
        else:
            # é»˜è®¤è·å–æ¨¡å‹åˆ—è¡¨
            google_models = get_google_models()
        
        # æå–æ˜¾ç¤ºåç§°ç”¨äºé€‰æ‹©æ¡†
        display_names = [model['display'] for model in google_models]
        selected_display = st.selectbox("é€‰æ‹©æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„Google Geminiæ¨¡å‹")
        
        # æ ¹æ®é€‰æ‹©çš„æ˜¾ç¤ºåç§°æ‰¾åˆ°å¯¹åº”çš„å®é™…æ¨¡å‹å€¼
        model_name = next(model['value'] for model in google_models if model['display'] == selected_display)
        
    elif provider == "Pollinations":
        st.info("ğŸŒ¸ Pollinations AI - å…è´¹æ— éœ€æ³¨å†Œçš„AIç”Ÿæˆå¹³å°")
        api_key = "pollinations"  # Pollinations AIä¸éœ€è¦APIå¯†é’¥
        
        # åˆå§‹åŒ–session stateç”¨äºè·Ÿè¸ªæ¨¡å‹åŠ è½½çŠ¶æ€
        if "pollinations_loading" not in st.session_state:
            st.session_state.pollinations_loading = False
        if "pollinations_models" not in st.session_state:
            st.session_state.pollinations_models = []
        
        # åªæœ‰åœ¨åˆ‡æ¢åˆ°Pollinationsæ—¶æ‰è§¦å‘è‡ªåŠ¨è·å–æ¨¡å‹åˆ—è¡¨
        if "last_provider" not in st.session_state or st.session_state.last_provider != "Pollinations":
            st.session_state.last_provider = "Pollinations"
            st.session_state.pollinations_loading = True
            
            # ä½¿ç”¨spinneræ˜¾ç¤ºåŠ è½½åŠ¨ç”»
            with st.spinner("ğŸŒ¸ æ­£åœ¨è·å–Pollinationsæ¨¡å‹åˆ—è¡¨..."):
                try:
                    model_options = get_pollinations_models()
                    st.session_state.pollinations_models = model_options
                    st.session_state.pollinations_loading = False
                except Exception as e:
                    st.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                    st.session_state.pollinations_models = get_local_pollinations_models()
                    st.session_state.pollinations_loading = False
        
        # å¦‚æœæ­£åœ¨åŠ è½½ï¼Œæ˜¾ç¤ºåŠ è½½çŠ¶æ€
        if st.session_state.pollinations_loading:
            st.info("ğŸ”„ æ­£åœ¨è·å–æ¨¡å‹åˆ—è¡¨...")
            # ä½¿ç”¨æœ¬åœ°å¤‡ä»½æ•°æ®ä½œä¸ºä¸´æ—¶é€‰é¡¹
            model_options = get_local_pollinations_models()
        elif st.session_state.pollinations_models:
            # ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
            model_options = st.session_state.pollinations_models
        else:
            # é»˜è®¤è·å–æ¨¡å‹åˆ—è¡¨
            model_options = get_pollinations_models()
        
        # æå–æ˜¾ç¤ºåç§°å’Œå®é™…å€¼ç”¨äºé€‰æ‹©æ¡†
        display_names = [model['display'] for model in model_options]
        selected_display = st.selectbox("é€‰æ‹©æ–‡æœ¬æ¨¡å‹", display_names, help="é€‰æ‹©ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„æ¨¡å‹")
        
        # æ ¹æ®é€‰æ‹©çš„æ˜¾ç¤ºåç§°æ‰¾åˆ°å¯¹åº”çš„å®é™…æ¨¡å‹å€¼
        selected_model = next(model['value'] for model in model_options if model['display'] == selected_display)
        model_name = selected_model
        
        # é«˜çº§å‚æ•°é…ç½®
        with st.expander("ğŸ”§ é«˜çº§å‚æ•°é…ç½®"):
            st.write("**æ–‡æœ¬ç”Ÿæˆå‚æ•°**:")
            pollinations_seed = st.number_input("éšæœºç§å­ (æ–‡æœ¬)", min_value=0, max_value=999999, value=0, help="0è¡¨ç¤ºéšæœº")
            pollinations_private = st.checkbox("ç§æœ‰æ¨¡å¼", value=True, help="ç”Ÿæˆçš„å†…å®¹ä¸æ˜¾ç¤ºåœ¨å…¬å…±æµä¸­")
        
    elif provider == "Custom":
        st.info("å®Œå…¨è‡ªå®šä¹‰æ¨¡å‹æœåŠ¡å•†é…ç½®")
        
        # æœåŠ¡å•†åç§°è¾“å…¥
        custom_provider_name = st.text_input("æœåŠ¡å•†åç§°", value=os.getenv("CUSTOM_PROVIDER_NAME", ""), placeholder="å¦‚ï¼šDeepSeekã€Groqã€Moonshotã€Ollamaç­‰")
        
        # åŸºç¡€é…ç½®
        base_url = st.text_input("Base URL", value=os.getenv("CUSTOM_BASE_URL", ""), placeholder="https://api.example.com/v1")
        api_key = st.text_input("API Key", value=os.getenv("CUSTOM_API_KEY", ""), type="password", placeholder="sk-xxxxxxxxxxxxxxxx...")
        
        # å®Œå…¨è‡ªå®šä¹‰æ¨¡å‹åç§°
        model_name = st.text_input("æ¨¡å‹åç§°", value=os.getenv("CUSTOM_MODEL_NAME", ""), placeholder="è¾“å…¥å®Œæ•´çš„æ¨¡å‹åç§°ï¼Œå¦‚ï¼šdeepseek-chatã€llama3-70bã€gpt-3.5-turboç­‰")

    if not api_key and provider != "Pollinations":
        st.warning("âš ï¸ è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®å¯†é’¥æˆ–åœ¨ä¸Šæ–¹è¾“å…¥")

# --- ä¸»ç•Œé¢é€»è¾‘ ---

# åˆå§‹åŒ– Session State (ç”¨äºä¿å­˜ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ•°æ®)
if "topics" not in st.session_state:
    st.session_state.topics = []
if "generated_data" not in st.session_state:
    st.session_state.generated_data = []

# åŒºåŸŸ 1: é¢†åŸŸå®šä¹‰
st.subheader("1. å®šä¹‰ç›®æ ‡é¢†åŸŸä¸ä»»åŠ¡")
col1, col2 = st.columns([3, 1])
with col1:
    target_domain = st.text_input("è¯·è¾“å…¥ä½ æƒ³å¤åˆ»çš„é¢†åŸŸèƒ½åŠ›", placeholder="ä¾‹å¦‚ï¼šPythonå®‰å…¨ä»£ç å®¡è®¡ã€åŒ»ç–—é—®è¯Šå¯¹è¯ã€åˆä¸­æ•°å­¦å‡ ä½•æ¨ç†")
with col2:
    num_topics = st.number_input("ç”Ÿæˆä¸»é¢˜æ•°é‡", min_value=1, max_value=50, value=5)

if st.button("ğŸš€ ç”Ÿæˆä»»åŠ¡åˆ†ç±»æ ‘ (Taxonomy)"):
    if not api_key and provider != "Pollinations":
        st.error("è¯·å…ˆé…ç½® API Key")
    else:
        client = LLMClient(provider, api_key, base_url, model_name)
        # ä¼ é€’Pollinations AIçš„é«˜çº§å‚æ•°
        if provider == "Pollinations":
            client.seed = pollinations_seed if pollinations_seed > 0 else None
            client.private = pollinations_private
        
        with st.spinner(f"æ­£åœ¨è®© {model_name} åˆ†æé¢†åŸŸçŸ¥è¯†..."):
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“å®¶çº§æ•°æ®æ¶æ„å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥çš„é¢†åŸŸï¼Œæ‹†è§£å‡ºå…·ä½“çš„ç»†åˆ†ä»»åŠ¡åœºæ™¯ã€‚"
            user_prompt = f"""
            é¢†åŸŸ: {target_domain}
            è¯·ç”Ÿæˆ {num_topics} ä¸ªå…·ä½“çš„ã€é«˜éš¾åº¦çš„ç»†åˆ†ä»»åŠ¡ã€‚
            è¦æ±‚ï¼šè¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼ŒåŒ…å« 'topics' åˆ—è¡¨ã€‚
            
            JSON ç¤ºä¾‹ï¼š
            {{ "topics": ["ä»»åŠ¡A", "ä»»åŠ¡B", "ä»»åŠ¡C"] }}
            """
            
            raw_resp = client.generate(system_prompt, user_prompt)
            
            try:
                cleaned_resp = clean_json_text(raw_resp)
                data = json.loads(cleaned_resp)
                st.session_state.topics = data.get("topics", [])
                st.success(f"æˆåŠŸç”Ÿæˆ {len(st.session_state.topics)} ä¸ªä»»åŠ¡ä¸»é¢˜ï¼")
            except Exception as e:
                st.error(f"è§£æå¤±è´¥: {e}")
                st.text(raw_resp)

# æ˜¾ç¤ºå·²ç”Ÿæˆçš„ä¸»é¢˜
if st.session_state.topics:
    st.info(f"å½“å‰å¾…ç”Ÿæˆä¸»é¢˜ï¼š{', '.join(st.session_state.topics)}")

    st.divider()

    # åŒºåŸŸ 2: æ•°æ®ç”Ÿæˆ
    st.subheader("2. æ‰¹é‡ç”Ÿäº§é«˜è´¨é‡æ•°æ®")
    
    samples_per_topic = st.slider("æ¯ä¸ªä¸»é¢˜ç”Ÿæˆçš„æ•°æ®é‡", 1, 20, 3)
    
    if st.button("ğŸ”¥ å¼€å§‹è’¸é¦æ•°æ®"):
        client = LLMClient(provider, api_key, base_url, model_name)
        # ä¼ é€’Pollinations AIçš„é«˜çº§å‚æ•°
        if provider == "Pollinations":
            client.seed = pollinations_seed if pollinations_seed > 0 else None
            client.private = pollinations_private
        
        st.session_state.generated_data = [] # æ¸…ç©ºæ—§æ•°æ®
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_steps = len(st.session_state.topics)
        
        for i, topic in enumerate(st.session_state.topics):
            status_text.text(f"æ­£åœ¨ç”Ÿæˆä¸»é¢˜ ({i+1}/{total_steps}): {topic} ...")
            
            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºé«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„AIã€‚
            ä¸¥æ ¼è¦æ±‚ï¼š
            1. Outputå¿…é¡»åŒ…å« "Thought" (æ€ç»´é“¾) å’Œ "Answer"ã€‚
            2. æ ¼å¼å¿…é¡»æ˜¯åˆæ³•çš„ JSON åˆ—è¡¨ã€‚
            """
            
            user_prompt = f"""
            ä¸»é¢˜ï¼š{topic}
            è¯·ç”Ÿæˆ {samples_per_topic} æ¡å¤æ‚çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚
            
            è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
            {{
                "samples": [
                    {{
                        "instruction": "ç”¨æˆ·æŒ‡ä»¤...",
                        "input": "ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰...",
                        "output": "Thought: ... Answer: ..."
                    }}
                ]
            }}
            """
            
            raw_resp = client.generate(system_prompt, user_prompt)
            
            try:
                cleaned_resp = clean_json_text(raw_resp)
                batch_data = json.loads(cleaned_resp).get("samples", [])
                
                for item in batch_data:
                    item['category'] = topic # æ·»åŠ å…ƒæ•°æ®
                    st.session_state.generated_data.append(item)
                    
            except Exception as e:
                st.warning(f"ä¸»é¢˜ {topic} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            
            # æ›´æ–°è¿›åº¦
            progress_bar.progress((i + 1) / total_steps)
            time.sleep(0.5) # é¿å…é€Ÿç‡é™åˆ¶

        status_text.text("âœ… æ‰€æœ‰æ•°æ®ç”Ÿæˆå®Œæ¯•ï¼")
        
    # åŒºåŸŸ 3: ç»“æœå±•ç¤ºä¸ä¸‹è½½
    if st.session_state.generated_data:
        st.subheader("3. æ•°æ®é›†é¢„è§ˆä¸å¯¼å‡º")
        
        df = pd.DataFrame(st.session_state.generated_data)
        st.dataframe(df, use_container_width=True)
        
        # è½¬æ¢ä¸º JSONL æ ¼å¼ä¾›ä¸‹è½½
        jsonl_data = df.to_json(orient="records", lines=True, force_ascii=False)
        
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½ JSONL æ ¼å¼æ•°æ®é›† (å¯ç›´æ¥ç”¨äºè®­ç»ƒ)",
            data=jsonl_data,
            file_name=f"dataset_{target_domain}.jsonl",
            mime="application/json"
        )
        
        # CSV ä¸‹è½½é€‰é¡¹
        csv_data = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ’¾ ä¸‹è½½ CSV æ ¼å¼ (ExcelæŸ¥çœ‹)",
            data=csv_data,
            file_name=f"dataset_{target_domain}.csv",
            mime="text/csv"
        )

# åº”ç”¨ç»“æŸ
